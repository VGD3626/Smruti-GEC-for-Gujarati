{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1749714243946,
     "user": {
      "displayName": "Vrund Dobariya",
      "userId": "02667739450293044164"
     },
     "user_tz": -330
    },
    "id": "odmqf4Wl57_N"
   },
   "outputs": [],
   "source": [
    "stopwords = [], []\n",
    "import re\n",
    "\n",
    "def WordTokenizer(data, corpus='poetry', keep_stopwords = True):\n",
    "        data = re.sub(r'([.,:;\\'\\\\\"!?%#@*<>|\\+\\-\\(\\)])', r' \\1 ', data)\n",
    "        # data = re.sub(r'')\n",
    "        data = re.sub(r\"”“\", r'\\\"', data)\n",
    "        data = re.sub(r'…', \" \", data)\n",
    "        data = re.split(r'[ -]',data)\n",
    "        words = []\n",
    "\n",
    "        if not keep_stopwords:\n",
    "            for word in data:\n",
    "                if word not in stopwords:\n",
    "                    words.append(word)\n",
    "            return words\n",
    "\n",
    "        for i in data:\n",
    "            if i:\n",
    "                words.append(i)\n",
    "        return words\n",
    "\n",
    "\n",
    "def SentenceTokenizer(data):\n",
    "    data = data.strip()\n",
    "    data = re.sub(r'([.!?])', r'\\1 ', data)\n",
    "    data = re.split(r'  ',data)\n",
    "    if not data[-1]:\n",
    "    \tdel(data[-1])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1749714244964,
     "user": {
      "displayName": "Vrund Dobariya",
      "userId": "02667739450293044164"
     },
     "user_tz": -330
    },
    "id": "Wlri2ClI5UrH"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Preprocessor():\n",
    "    def __init__(self):\n",
    "        self.suffixes = []\n",
    "        pass\n",
    "\n",
    "    def compulsory_preprocessing(self, text):\n",
    "        '''This is a function to preprocess the text and make the necessary changes which are compulsory for any type of Gujarati NLP task'''\n",
    "        text = re.sub(r'\\u200b', '', text)\n",
    "        text = re.sub(r'\\ufeff', \"\", text)\n",
    "        text = re.sub(r'…', \" \", text)\n",
    "        text = re.sub(r'  ', ' ', text)\n",
    "        text = re.sub(r'”“', '', text)\n",
    "        text = WordTokenizer(text)\n",
    "        for i in range(len(text)):\n",
    "            text[i] = text[i].rstrip(':')\n",
    "        return ' '.join(text)\n",
    "\n",
    "    def remove_tek(self, text, tek_string):\n",
    "        '''\n",
    "        Tek is the Gujarati word for the initial line of the poem. Whenever, one stanza of any poem is sung, the initial line of the poem is sung once again before starting the\n",
    "        next stanza. This is called as singing a \"Tek\". Written poems mention the tek string too many a times. This will cause a problem of redundancy. Hence, removing it is\n",
    "        necessary.\n",
    "        '''\n",
    "        if str(type(tek_string))==\"<class 'NoneType'>\" or not tek_string:\n",
    "            raise TypeError('tek_string needs to be a valid string')\n",
    "        if str(type(text))==\"<class 'list'>\":\n",
    "            for i in range(len(text)):\n",
    "                text[i] = text[i].rstrip(tek_string)\n",
    "        elif str(type(text))==\"<class 'str'>\":\n",
    "            text = text.rstrip(tek_string)\n",
    "        else:\n",
    "            raise TypeError(\"Argument 'text' must be either a str or list\")\n",
    "        return text\n",
    "\n",
    "    def poetic_preprocessing(self, text, remove_tek=False, tek_string=None):\n",
    "        '''This function is only required when dealing with poetic corpora. Make sure to use this function along with the compulsory preprocessing to have decently accurate results with poetic corpora'''\n",
    "        text = re.sub(r'।','.',text)\n",
    "        text = re.sub(' ।।[૧૨૩૪૫૬૭૮૯૦]।।', '.', text)\n",
    "        if remove_tek:\n",
    "            text = self.remove_tek(text, tek_string)\n",
    "        tokens = WordTokenizer(text, corpus='poetry', keep_punctuations=False)\n",
    "\n",
    "        for i in range(len(tokens)):\n",
    "            # Rule 1\n",
    "            if tokens[i].endswith('જી'):\n",
    "                tokens[i] = tokens[i].strip('જી')\n",
    "            # Rule 2\n",
    "            if tokens[i].endswith('ૈ'):\n",
    "                tokens[i] = tokens[i].strip('ૈ')+'ે'\n",
    "            # Rule 3\n",
    "            index = tokens[i].find('ર')\n",
    "            if index == -1:\n",
    "                pass\n",
    "            elif index<len(tokens[i])-1 and tokens[i][index-1]=='િ':\n",
    "                tokens[i] = re.sub('િર', 'ૃ', tokens[i])\n",
    "\n",
    "        return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1749714379174,
     "user": {
      "displayName": "Vrund Dobariya",
      "userId": "02667739450293044164"
     },
     "user_tz": -330
    },
    "id": "cnA3ptW748ci"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# You may need to add/remove suffixes/prefixes according to the corpora\n",
    "suffixes = ['નાં','ના','ની','નો','નું','ને','થી','માં','એ','ીએ','ઓ','ે','તા','તી','વા','મા','વું','વુ','ો','માંથી','શો','ીશ','ીશું','શે',\n",
    "\t\t\t'તો','તું','તાં','્યો','યો','યાં','્યું','યું','ોઈશ', 'ોઈશું', '્યા','યા','્યાં','સ્વી','રે','ં','મ્','મ્','ી','કો',\n",
    "      'ેલ', 'ેલો', 'ેલા', 'ેલું', 'ેલી', 'ણે', 'ણા', 'ણું', 'ણો', 'ણી'\n",
    "      ]\n",
    "prefixes = [] #['અ']\n",
    "class Stemmer():\n",
    "\tdef __init__(self):\n",
    "\t\tself.suffixes = suffixes\n",
    "\t\tself.prefixes = prefixes\n",
    "\n",
    "\tdef add_suffix(self, suffix):\n",
    "\t\tself.suffixes.append(suffix)\n",
    "\n",
    "\tdef add_prefix(self, prefix):\n",
    "\t\tself.prefixes.append(prefix)\n",
    "\n",
    "\tdef delete_suffix(self, suffix):\n",
    "\t\ttry:\n",
    "\t\t\tdel(self.suffixes[self.suffixes.index(suffix)])\n",
    "\t\texcept IndexError:\n",
    "\t\t\tprint('{} not present in suffixes'.format(suffix))\n",
    "\n",
    "\tdef delete_prefix(self, prefix):\n",
    "\t\ttry:\n",
    "\t\t\tdel(self.prefixes[self.prefixes.index(prefix)])\n",
    "\t\texcept IndexError:\n",
    "\t\t\tprint(\"{} not present in prefixes\".format(prefix))\n",
    "\n",
    "\n",
    "\tdef stem_word(self, sentence, corpus):\n",
    "\t\tword_list = sentence.strip('\\u200b').split(' ')\n",
    "\t\tif not word_list[-1]:\n",
    "\t\t\tdel(word_list[-1])\n",
    "\t\treturn_list = []\n",
    "\t\tsuffix_list = []\n",
    "\t\tpuctuations = ('.',',','!','?','\"',\"'\",'%','#','@','&','…','“', '”', '’', '‘', ':', ';')\n",
    "\t\tfor word in word_list:\n",
    "\t\t\ta = word\n",
    "\t\t\tremoved_suffix = None\n",
    "\t\t\tif word.endswith(puctuations):\n",
    "\t\t\t\ta = word[:-1]\n",
    "\n",
    "\t\t\tfor suffix in suffixes:\n",
    "\t\t\t\tif a.endswith(suffix):\n",
    "\t\t\t\t\ta = a[:-len(suffix)]\n",
    "\t\t\t\t\tremoved_suffix = suffix\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\tfor prefix in prefixes:\n",
    "\t\t\t\tif a.startswith(prefix):\n",
    "\t\t\t\t\ta = a[len(prefix):]\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\tif word.endswith(puctuations):\n",
    "\t\t\t\ta += str(word[-1])\n",
    "\t\t\treturn_list.append(a)\n",
    "\t\t\tsuffix_list.append(removed_suffix)\n",
    "\t\treturn_sentence = \" \".join(return_list)\n",
    "\t\treturn {\n",
    "\t\t\t\"stemmed_sentence\": return_sentence,\n",
    "\t\t\t\"removed_suffixes\": suffix_list\n",
    "\t\t}\n",
    "\n",
    "\tdef stem(self, text, corpus='prose', remove_tek=False, tek_string=None):\n",
    "\t\tpreprocessor = Preprocessor()\n",
    "\t\ttext = preprocessor.compulsory_preprocessing(text)\n",
    "\t\tif corpus == 'poetry':\n",
    "\t\t\ttext = preprocessor.poetic_preprocessing(text, remove_tek=remove_tek, tek_string=tek_string)\n",
    "\t\telif corpus == 'prose':\n",
    "\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\traise ValueError(\"Unnrecognized argument 'corpus'. Should be either 'prose' or 'poetry'\")\n",
    "\t\tl = SentenceTokenizer(text)\n",
    "\t\tif len(l)==1:\n",
    "\t\t\tsentence = l[0]\n",
    "\t\t\treturn self.stem_word(sentence, corpus=corpus)\n",
    "\t\telse:\n",
    "\t\t\ta = []\n",
    "\t\t\tfor sentence in l:\n",
    "\t\t\t\ta.append(self.stem(sentence))\n",
    "\t\t\treturn a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1749714379448,
     "user": {
      "displayName": "Vrund Dobariya",
      "userId": "02667739450293044164"
     },
     "user_tz": -330
    },
    "id": "GO7HMtDc6Dcc",
    "outputId": "0a6967c7-11f4-4125-d709-74b539a56d34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stemmed_sentence': 'એમાં બોધ તારવવા મગજમાર કરવા માર કશ જરૂર ન ,',\n",
       " 'removed_suffixes': ['થી', None, 'ની', 'ી', 'ની', 'ે', 'ી', None, 'થી', None]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stmr = Stemmer()\n",
    "stmr.stem(\"એમાંથી બોધ તારવવાની મગજમારી કરવાની મારે કશી જરૂર નથી,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 149,
     "status": "ok",
     "timestamp": 1749714379847,
     "user": {
      "displayName": "Vrund Dobariya",
      "userId": "02667739450293044164"
     },
     "user_tz": -330
    },
    "id": "6gvQ1Ni76did"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open('/content/drive/MyDrive/Gujarati_Spelling_and_Grammar_Autocorrect/data/15k_sampled_sentences.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "df = pd.DataFrame({'sentence': lines})\n",
    "df['n_err'] = 0\n",
    "df['err_sentence'] = df['sentence']\n",
    "df['err_types'] = \"\"\n",
    "\n",
    "def load_word_list(path):\n",
    "    with open(path, encoding=\"utf-8\") as word_list:\n",
    "        return set([word.strip() for word in word_list])\n",
    "\n",
    "vocab = load_word_list(\"/content/drive/MyDrive/Gujarati_Spelling_and_Grammar_Autocorrect/hunspell-gu.dic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1749714980414,
     "user": {
      "displayName": "Vrund Dobariya",
      "userId": "02667739450293044164"
     },
     "user_tz": -330
    },
    "id": "ROOObMN0A7eG"
   },
   "outputs": [],
   "source": [
    "suffix_classes = {\n",
    "    \"class1\": ['નાં','ના','ની','નો','નું','ને','નુ'],\n",
    "    \"class2\": ['માં','એ','ઓ','માંથી'],\n",
    "    \"class3\": ['વા','વું','વુ'],\n",
    "    \"class4\": ['શો','ીશ','ીશું','શે', 'ીશુ'],\n",
    "    \"class5\": ['તો','તું','તાં','તા','તી'],\n",
    "    \"class6\": ['્્યો','યો','યાં','્યું','યું','્યા','યા','્યાં'],\n",
    "    \"class7\": ['સ્વી','રે','મ્'],\n",
    "    \"class8\": ['ણે', 'ણા', 'ણું', 'ણો', 'ણી'],\n",
    "    # \"class8\": ['ી','ો','ે','ું'],\n",
    "    \"class9\": ['ેલ', 'ેલો', 'ેલા', 'ેલું', 'ેલી''ેલુ'],\n",
    " }\n",
    "\n",
    "aux_suffix_classes = {\n",
    "    \"class9\" : ['તો','તું','તાં','તા','તી','શો', 'ોઈશ', 'ોઈશું','શે'], #based on tense (only for aux હ)\n",
    "    \"class10\" : ['ે', 'ું', 'ીએ', 'ો']  #based on tense (only for aux છ)\n",
    "}\n",
    "\n",
    "aux_stemmed =  ['હ', 'છ']\n",
    "\n",
    "punctuations = ['.', ',', '!', '?', '\"', \"'\", ':', ';']\n",
    "matra_classes = {\n",
    "    \"class1\": [\"ુ\", \"ૂ\"],\n",
    "    \"class2\": [\"િ\", \"ી\"],\n",
    "    \"class3\": [\"ે\", \"ૈ\"],\n",
    "    \"class7\":[\"ઉ\", \"ઊ\"],\n",
    "    \"class4\": [\"ં\"],\n",
    "    \"class5\": [\"ો\", \"ૌ\"],\n",
    "    \"class6\": [\"ૃ\", \"ૄ\"],\n",
    "    \"class7\": [\"ં\"],\n",
    "    \"class9\": [\"ં\"]\n",
    "}\n",
    "\n",
    "\n",
    "def is_anusvara_related_change(s1, s2):\n",
    "    return ('ં' in s1) != ('ં' in s2) and s1.count('ં') != s1.count('ં')\n",
    "\n",
    "def gen_punct_err(row):\n",
    "    row = row.copy()\n",
    "    tokens = WordTokenizer(row['err_sentence'])\n",
    "    punct_indices = [i for i, tok in enumerate(tokens) if tok in punctuations]\n",
    "\n",
    "    if not punct_indices:\n",
    "        return False, row\n",
    "\n",
    "    idx = random.choice(punct_indices)\n",
    "    current = tokens[idx]\n",
    "    choices = [p for p in punctuations if p != current and p!='!']\n",
    "    choices.append(\"\")\n",
    "    tokens[idx] = random.choice(choices)\n",
    "    row['sentence'] = ' '.join(tokens)\n",
    "    row['err_types'] = row['err_types']+\"ORTH:PUNCT \"\n",
    "    # print(row[\"sentence\"])\n",
    "    return True, row\n",
    "\n",
    "\n",
    "def gen_morph_err(row):\n",
    "    row = row.copy()\n",
    "    tokens = WordTokenizer(row['err_sentence'])\n",
    "    if not tokens:\n",
    "        return False, row\n",
    "\n",
    "    idx = random.randint(0, len(tokens) - 1)\n",
    "    token = tokens[idx]\n",
    "    if token in ('અને', 'તને'):\n",
    "      return False, row\n",
    "    # print(\"b:\", token)\n",
    "    stemmer = Stemmer()\n",
    "    stem_result = stemmer.stem_word(token, corpus='prose')\n",
    "    stemmed = stem_result[\"stemmed_sentence\"]\n",
    "    if stem_result[\"removed_suffixes\"]:\n",
    "      removed_suffix = stem_result[\"removed_suffixes\"][0]\n",
    "    else:\n",
    "      return False, row\n",
    "\n",
    "    if removed_suffix is None or stemmed == token or stemmed == \"\":\n",
    "        return False, row\n",
    "\n",
    "    if stemmed == aux_stemmed[0] or stemmed == aux_stemmed[1]:\n",
    "\n",
    "        x = random.randint(0, 1)\n",
    "        if x:\n",
    "          idy = random.randint(0, len(aux_suffix_classes['class9'])-1)\n",
    "          if removed_suffix == aux_suffix_classes['class9'][idy]:\n",
    "              idy = (idy + 1) % len(aux_suffix_classes['class9'])\n",
    "\n",
    "          tokens[idx] = aux_stemmed[0] + aux_suffix_classes['class9'][idy]\n",
    "          # print(\"a:\", tokens[idx])\n",
    "          if tokens[idx] in vocab:\n",
    "            row['sentence'] = ' '.join(tokens)\n",
    "            row['err_types'] = row['err_types']+\"MORPH \"\n",
    "            return True, row\n",
    "          return False, row\n",
    "\n",
    "        else:\n",
    "          idy = random.randint(0, len(aux_suffix_classes['class10'])-1)\n",
    "          if removed_suffix == aux_suffix_classes['class10'][idy]:\n",
    "              idy = (idy + 1) % len(aux_suffix_classes['class10'])\n",
    "\n",
    "          tokens[idx] = aux_stemmed[1] + aux_suffix_classes['class10'][idy]\n",
    "          # print(\"a:\", tokens[idx])\n",
    "          if tokens[idx] in vocab:\n",
    "            row['sentence'] = ' '.join(tokens)\n",
    "            row['err_types'] = row['err_types']+\"MORPH \"\n",
    "            return True, row\n",
    "          return False, row\n",
    "\n",
    "    class_keys = list(suffix_classes.keys())\n",
    "    random.shuffle(class_keys)\n",
    "    for cls in class_keys:\n",
    "        if removed_suffix in suffix_classes[cls]:\n",
    "            idy = random.randint(0, len(suffix_classes[cls]) - 1)\n",
    "\n",
    "            if removed_suffix == suffix_classes[cls][idy]:\n",
    "                idy = (idy + 1) % len(suffix_classes[cls])\n",
    "\n",
    "            tokens[idx] = stemmed + suffix_classes[cls][idy]\n",
    "            # print(\"a:\", tokens[idx])\n",
    "            row['sentence'] = ' '.join(tokens)\n",
    "\n",
    "            if is_anusvara_related_change(tokens[idx], token):\n",
    "              row['err_types'] = row['err_types']+\"SPELL:ANUSVARA \"\n",
    "            else:\n",
    "              row['err_types'] = row['err_types']+\"MORPH \"\n",
    "            # print(row[\"sentence\"])\n",
    "            if tokens[idx] in vocab:\n",
    "              return True, row\n",
    "    return False, row\n",
    "\n",
    "def gen_synt_err(row):\n",
    "    row = row.copy()\n",
    "    tokens = WordTokenizer(row['err_sentence'])\n",
    "    if len(tokens) < 2:\n",
    "        return False, row\n",
    "\n",
    "    indices = [i for i in range(len(tokens) - 1) if tokens[i] not in punctuations and tokens[i + 1] not in punctuations]\n",
    "\n",
    "    if not indices:\n",
    "        return False, row\n",
    "\n",
    "    idx = random.choice(indices)\n",
    "    if tokens[idx] != tokens[idx + 1]:\n",
    "      tokens[idx], tokens[idx + 1] = tokens[idx + 1], tokens[idx]\n",
    "    else:\n",
    "      return False, row\n",
    "    row['sentence'] = ' '.join(tokens)\n",
    "    # print(row[\"sentence\"])\n",
    "    row['err_types'] = row['err_types']+\"SYNT:WO \"\n",
    "    return True, row\n",
    "\n",
    "def gen_spell_err(row):\n",
    "    row = row.copy()\n",
    "    tokens = WordTokenizer(row['err_sentence'])\n",
    "    if not tokens:\n",
    "        return False, row\n",
    "\n",
    "    idx = random.randint(0, len(tokens) - 1)\n",
    "    token = tokens[idx]\n",
    "    class_keys = list(matra_classes.keys())\n",
    "    random.shuffle(class_keys)\n",
    "\n",
    "    for cls in class_keys:\n",
    "        for matra in matra_classes[cls]:\n",
    "\n",
    "            if matra not in token:\n",
    "                continue\n",
    "            choices = [m for m in matra_classes[cls] if m != matra]\n",
    "            # print(matra, choices)\n",
    "            if not choices and cls not in [\"class4\", \"class9\", \"class7\"]:\n",
    "                return False, row\n",
    "\n",
    "            replacement = None\n",
    "            if cls in [\"class4\", \"class9\",\"class7\"]:\n",
    "                choices.append(\"\")\n",
    "                replacement = random.choice(choices)\n",
    "                row['err_types'] += \"SPELL:ANUSVARA \"\n",
    "            else:\n",
    "                replacement = random.choice(choices)\n",
    "                row['err_types'] += \"SPELL:MATRA \"\n",
    "\n",
    "            tokens[idx] = token.replace(matra, replacement, 1)\n",
    "            # print(tokens[idx], replacement, matra)\n",
    "            row['sentence'] = ' '.join(tokens)\n",
    "            # print(row['sentence'])\n",
    "            return True, row\n",
    "\n",
    "    return False, row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1749714986444,
     "user": {
      "displayName": "Vrund Dobariya",
      "userId": "02667739450293044164"
     },
     "user_tz": -330
    },
    "id": "cTrsPZ2F_EeY"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def generate_errs(df, prob_list=[0, 1, 0, 0], max_err_per_sentence=3):\n",
    "    funcs = [\n",
    "        gen_punct_err,\n",
    "        gen_morph_err,\n",
    "        gen_synt_err,\n",
    "        gen_spell_err\n",
    "    ]\n",
    "\n",
    "    # if int(sum(prob_list)) != 1:\n",
    "    #     raise ValueError(\"Probabilities must sum to 1\")\n",
    "\n",
    "    n_samples = len(df)\n",
    "    weights = np.array(prob_list)\n",
    "    weights = weights / weights.sum()\n",
    "    chosen_func = random.choices(funcs, weights=weights, k=1)[0]\n",
    "\n",
    "\n",
    "    # valid_rows = df[df['n_err'] < max_err_per_sentence]\n",
    "    # if valid_rows.empty:\n",
    "    #   print(\"Max errors reached for all the sentences.\")\n",
    "    #   return False, None\n",
    "    # row = valid_rows.sample(1).iloc[0]\n",
    "    # print(f\"Selected row with current error count: {row['n_err']}\")\n",
    "\n",
    "\n",
    "    result = False, None\n",
    "\n",
    "    i = 0\n",
    "    while not result[0]:\n",
    "        i += 1\n",
    "        if n_samples == i:\n",
    "            print(\"unable to generate error\")\n",
    "            break\n",
    "\n",
    "        valid_rows = df[df['n_err'] == 0]\n",
    "        # valid_rows = df[df['n_err'] < max_err_per_sentence]\n",
    "        if valid_rows.empty:\n",
    "          print(\"Max errors reached for all the sentences.\")\n",
    "          return False, None\n",
    "        row = valid_rows.sample(1).iloc[0]\n",
    "        result = chosen_func(row)\n",
    "        idx = row.name\n",
    "\n",
    "\n",
    "    if result[0]:\n",
    "        df.at[idx, 'n_err'] += 1\n",
    "        df.at[idx, 'err_sentence'] = result[1]['sentence']\n",
    "        df.at[idx, 'err_types'] = result[1]['err_types']\n",
    "\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 78541,
     "status": "ok",
     "timestamp": 1749715072749,
     "user": {
      "displayName": "Vrund Dobariya",
      "userId": "02667739450293044164"
     },
     "user_tz": -330
    },
    "id": "2IVzSQXnAxqL"
   },
   "outputs": [],
   "source": [
    "for _ in range(5000):\n",
    "  x = generate_errs(df, prob_list= [0.1,0.65,0.10,0.15], max_err_per_sentence= 3)#[0.05,0.45,0.20,0.30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74,
     "status": "ok",
     "timestamp": 1749715125264,
     "user": {
      "displayName": "Vrund Dobariya",
      "userId": "02667739450293044164"
     },
     "user_tz": -330
    },
    "id": "2Vb6YIeuBFnm",
    "outputId": "46540da8-4a84-4eb5-9c11-8541b76af914"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              n_err\n",
      "count  15200.000000\n",
      "mean       1.315789\n",
      "std        0.729309\n",
      "min        0.000000\n",
      "25%        1.000000\n",
      "50%        1.000000\n",
      "75%        2.000000\n",
      "max        3.000000\n"
     ]
    }
   ],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KeOXtsvnpbdx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNr27B6xXiXgXd8/ugCmEfz",
   "mount_file_id": "12OGayiorFTZRervgzurh7UkAZjGyz8or",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
