{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plVRzGvD1BuB"
   },
   "source": [
    "# morph pos model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAAgVtM0VePW"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet transformers==4.37\n",
    "!pip install --quiet sentence-transformers==2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j96eL86BCRb4"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1749198435733,
     "user": {
      "displayName": "Vrund Dobariya",
      "userId": "02667739450293044164"
     },
     "user_tz": -330
    },
    "id": "rPiM9J22VYKx",
    "outputId": "8bc252e9-8f0f-4548-a423-6ab97bedf42d"
   },
   "outputs": [],
   "source": [
    "NA='NA'\n",
    "MAX_LENGTH=120\n",
    "base_path='/content/drive/MyDrive/AI-ML-NLP/NLP/models/'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1749198501320,
     "user": {
      "displayName": "Vrund Dobariya",
      "userId": "02667739450293044164"
     },
     "user_tz": -330
    },
    "id": "zTnj1yYmWnCL",
    "outputId": "e345919e-b305-4ef1-c89f-e46eef6ea76f"
   },
   "outputs": [],
   "source": [
    "all_feature_values={\n",
    "    'pos':[\n",
    "      NA,'DM_DMI', 'CC_CCD', 'PSP', 'PR_PRQ', 'RP_RPD', 'PR_PRP', 'DM_DMQ', 'RB', 'QT_QTO', 'JJ', 'RD_ECH', 'PR_PRF', 'N_NNP', 'N_NN', 'RP_CL', 'V_VM', 'DM_DMD', 'RP_INTF', 'QT_QTC', 'RP_INJ', 'PR_PRC', 'V_VAUX_VNP', 'RD_PUNC', 'PR_PRI', 'PR_PRL', 'DM_DMR', 'CC_CCS_UT', 'RD_RDF', 'N_NST', 'RP_NEG', 'RD_SYM', 'V_VAUX', 'QT_QTF', 'CC_CCS', 'Value'\n",
    "    ],\n",
    "    'gender':[\n",
    "        NA,'MASC','FEM','NEUT'\n",
    "    ],\n",
    "    'number':[\n",
    "        NA,'SG','PL'\n",
    "    ],\n",
    "    'type':[\n",
    "        NA,'LGSPEC02','LGSPEC01','LGSPEC03'\n",
    "    ],\n",
    "    'person':[\n",
    "        NA,'1','2','3'\n",
    "    ],\n",
    "    'tense':[\n",
    "        NA,'PST','FUT'\n",
    "        # is present tense is not there?\n",
    "    ],\n",
    "    'case':[\n",
    "        NA,'ERG', 'GEN', 'NOM', 'DAT', 'LOC','ABL'\n",
    "    ],\n",
    "    'aspect':[\n",
    "        NA,'NFIN'\n",
    "    ],\n",
    "    # what is NFIN\n",
    "}\n",
    "\n",
    "feature_values={\n",
    "    'pos':[\n",
    "      NA,'DM_DMI', 'CC_CCD', 'PSP', 'PR_PRQ', 'RP_RPD', 'PR_PRP', 'DM_DMQ', 'RB', 'QT_QTO', 'JJ', 'RD_ECH', 'PR_PRF', 'N_NNP', 'N_NN', 'RP_CL', 'V_VM', 'DM_DMD', 'RP_INTF', 'QT_QTC', 'RP_INJ', 'PR_PRC', 'V_VAUX_VNP', 'RD_PUNC', 'PR_PRI', 'PR_PRL', 'DM_DMR', 'CC_CCS_UT', 'RD_RDF', 'N_NST', 'RP_NEG', 'RD_SYM', 'V_VAUX', 'QT_QTF', 'CC_CCS', 'Value'\n",
    "    ],\n",
    "    'gender':[\n",
    "        NA,'MASC','FEM','NEUT'\n",
    "    ],\n",
    "    'number':[\n",
    "        NA,'SG','PL'\n",
    "    ],\n",
    "    'type':[\n",
    "        NA,'LGSPEC02','LGSPEC01','LGSPEC03'\n",
    "    ],\n",
    "    'person':[\n",
    "        NA,'1','2','3'\n",
    "    ],\n",
    "    'tense':[\n",
    "        NA,'PST','FUT'\n",
    "        # is present tense is not there?\n",
    "    ],\n",
    "    'case':[\n",
    "        NA,'ERG', 'GEN', 'NOM', 'DAT', 'LOC','ABL'\n",
    "    ],\n",
    "    'aspect':[\n",
    "        NA,'NFIN'\n",
    "    ],\n",
    "    # what is NFIN\n",
    "}\n",
    "\n",
    "feature_seq=list(feature_values.keys())\n",
    "EXTRA_TOKEN=[-100]*len(feature_seq)\n",
    "\n",
    "total_number_of_features=0\n",
    "feature_value2id={}\n",
    "feature_id2value={}\n",
    "feature_start_range={}\n",
    "\n",
    "start_range=0\n",
    "for key,values in feature_values.items():\n",
    "  feature_value2id[key]={}\n",
    "  feature_start_range[key]=start_range\n",
    "  for i,value in enumerate(values):\n",
    "    feature_value2id[key][value]=i+start_range\n",
    "  feature_id2value[key]={(y-start_range):x for x,y in feature_value2id[key].items()}\n",
    "  start_range+=len(values)\n",
    "  total_number_of_features+=len(values)\n",
    "\n",
    "print(feature_value2id)\n",
    "print(feature_id2value)\n",
    "print(total_number_of_features)\n",
    "print(feature_start_range)\n",
    "\n",
    "\n",
    "number_of_labels=total_number_of_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "I5P3BjaZWoCK"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"l3cube-pune/gujarati-bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pF3DHzbmW9hL"
   },
   "outputs": [],
   "source": [
    "class CustomTokenClassificationModel(nn.Module):\n",
    "    def __init__(self, bert_model, feature_seq):\n",
    "        super(CustomTokenClassificationModel, self).__init__()\n",
    "        self.bert_model = bert_model\n",
    "\n",
    "        self.module_list = nn.ModuleList()\n",
    "\n",
    "        for key in feature_seq:\n",
    "          num_classes = len(feature_values[key])\n",
    "          module = nn.Linear(number_of_labels, num_classes)\n",
    "          self.module_list.append(module)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # 1) Run BERT to get last_hidden_state (shape: [B, L, 768])\n",
    "        outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.logits\n",
    "\n",
    "        # 2) Run each head on the 768‐dim embeddings\n",
    "        logits_list = []\n",
    "        for head in self.module_list:\n",
    "            logits = head(hidden_states)\n",
    "            # logits has shape [batch_size, seq_len, num_classes_for_that_feature]\n",
    "            logits_list.append(logits)\n",
    "\n",
    "        # 3) Return the list of all feature‐head logits\n",
    "        return logits_list\n",
    "\n",
    "class PosMorphClassificationModel(nn.Module):\n",
    "    def __init__(self, custom_model, feature_seq):\n",
    "        super(PosMorphClassificationModel, self).__init__()\n",
    "        self.custom_model = custom_model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.custom_model(\n",
    "              input_ids,\n",
    "              attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MorphAnalysis:\n",
    "    def __init__(self, tokenizer, inference_model, feature_seq, feature_id2value, max_length,NA):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inference_model = inference_model\n",
    "        self.feature_seq = feature_seq\n",
    "        self.feature_id2value = feature_id2value\n",
    "        self.max_length = max_length\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.NA = NA\n",
    "\n",
    "    def prepare_mask(self, word_ids):\n",
    "        mask = []\n",
    "        last = None\n",
    "        for i in word_ids:\n",
    "            if i is None or i == last:\n",
    "                mask.append(0)\n",
    "            else:\n",
    "                mask.append(1)\n",
    "            last = i\n",
    "        return mask\n",
    "\n",
    "    def tokenize_sentence(self, sentence, splitted=False):\n",
    "        if not splitted:\n",
    "            tokens = sentence.split(' ')\n",
    "        else:\n",
    "            tokens = sentence\n",
    "\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            tokens,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            is_split_into_words=True,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "        mask = self.prepare_mask(tokenized_inputs.word_ids(0))\n",
    "        sample = {\n",
    "            \"tokens\": tokens,\n",
    "            \"mask\": mask,\n",
    "            \"input_ids\": tokenized_inputs['input_ids'],\n",
    "            \"attention_mask\": tokenized_inputs['attention_mask']\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "    def prepare_output(self, sample):\n",
    "        tokens = sample['tokens']\n",
    "        output = []\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "          features = {}\n",
    "          for feat in self.feature_seq:\n",
    "            feat_val = sample[feat][i]\n",
    "            if feat_val != self.NA:\n",
    "              features[feat] = feat_val\n",
    "          output.append((token, features))\n",
    "        return output\n",
    "\n",
    "    def infer(self, sentence):\n",
    "        batch = self.tokenize_sentence(sentence)\n",
    "\n",
    "        input_ids = torch.tensor([batch[\"input_ids\"]]).to(self.device)\n",
    "        attention_mask = torch.tensor([batch[\"attention_mask\"]]).to(self.device)\n",
    "        mask = torch.tensor([batch['mask']]).to(self.device)\n",
    "\n",
    "        logits_list = self.inference_model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        curr_sample = {\n",
    "            \"tokens\": batch[\"tokens\"],\n",
    "        }\n",
    "\n",
    "        for i, logits in enumerate(logits_list):\n",
    "            key = self.feature_seq[i]\n",
    "            curr_mask = (mask != 0)\n",
    "            valid_logits = logits[curr_mask]\n",
    "            probabilities = F.softmax(valid_logits, dim=-1)\n",
    "            valid_predicted_labels = torch.argmax(probabilities, dim=-1)\n",
    "            curr_id2value_map = self.feature_id2value[key]\n",
    "            curr_sample[key] = [curr_id2value_map[x] for x in valid_predicted_labels.tolist()]\n",
    "\n",
    "\n",
    "        output = self.prepare_output(curr_sample)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxFngvPJ-tcY"
   },
   "outputs": [],
   "source": [
    "# prompt: Write a simple tokenizer\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    \"\"\"\n",
    "    A simple function to tokenize a string into a list of words.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of words (tokens).\n",
    "    \"\"\"\n",
    "    return text.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 5571,
     "status": "ok",
     "timestamp": 1749199171516,
     "user": {
      "displayName": "Vrund Dobariya",
      "userId": "02667739450293044164"
     },
     "user_tz": -330
    },
    "id": "1tiRN9q_XIYs",
    "outputId": "5370d276-f360-42f9-ab9e-94e924172c60"
   },
   "outputs": [],
   "source": [
    "inference_input_file_path=\"/content/drive/MyDrive/NLP Gujarati POS & Morph Analysis/POS_MORPH_MODEL/trained_model_binary_file/GUJ_POS_MORPH_ANAYLISIS_WRAPPER-v6.0-model.pth\"\n",
    "inference_checkpoint_path = inference_input_file_path\n",
    "\n",
    "inference_model=torch.load(inference_checkpoint_path,map_location=device, weights_only=False)\n",
    "inference_model.eval()\n",
    "inference_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_aR4XQCtXtr6"
   },
   "outputs": [],
   "source": [
    "model = MorphAnalysis(tokenizer, inference_model, feature_seq, feature_id2value, MAX_LENGTH,NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 24509,
     "status": "ok",
     "timestamp": 1749205803325,
     "user": {
      "displayName": "Vrund Dobariya",
      "userId": "02667739450293044164"
     },
     "user_tz": -330
    },
    "id": "Y0-Df6yHYtcD",
    "outputId": "4e95aa61-0f57-4746-e261-4576c8bc9af8"
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"બાળકો ક્રિકેટનો ખેલ રમી રહ્યા છે\",\n",
    "    \"તેમણે મને ખેલ કર્યો\",\n",
    "    \"મહેરબાની કરીને આ કામ કરો\",\n",
    "    \"મારે એક નવો કરો મળી ગયો છે\",\n",
    "    \"હું ભાત પાકીશ\",\n",
    "    \"આ પાક ખૂબ જ સ્વાદિષ્ટ છે\",\n",
    "    \"વિરાટ ક્રિકેટર છે\",\n",
    "    \"આ કાર્ય ખુબ જ વિરાટ હતું\",\n",
    "    \"રમેશ તું વિચાર\",\n",
    "    \"વિચાર આજે ગૃહકાર્ય નથી લાવ્યો\",\n",
    "    \"સ્વામી રાજી થયા\",\n",
    "    \"રાજી એ શાક ખાધું\",\n",
    "    \"બાળકો હરિત હસ્તો લહેવાનો ઇચ્છુક છે\",\n",
    "    \"હરિત પર્વાઓ એટલે બહારમાં વસ્ત્રો ધરાવવાનો આનંદ મને મળે છે\",\n",
    "    \"મેં નેતાગીરી પોતાની પાસે રાખી\",\n",
    "    \"લક્ષ્મીજીએ બલી રાજા ને રાખી બાંધી\",\n",
    "    \"તું મને મોબાઈલ નીચે મૂક\",\n",
    "    \"અકિરાને મૂક ભાષા આવડતી હતી\",\n",
    "    \"ગઈકાલે ક્રિષ્ણા આવી\",\n",
    "    \"ગઈકાલે ક્રિષ્ણા આવ્યો\",\n",
    "    \"મને શ્રી કૃષ્ણ ની વાતો ગમે\",\n",
    "    \"કનૈયો કૃષ્ણ વર્ણનો હોવા છતાં ખુબ જ મોહક લાગે\",\n",
    "    \"માછીમારે માછલીને મારી નાખી\",\n",
    "    \"મારી માછલી મેં તળાવમાં નાખી\",\n",
    "    \"મેં તળાવમાં મારી માછલી નાખી\",\n",
    "    \"મેં તળાવમાં માછલી મારી નાખી\",\n",
    "    \"તમે શાંતિથી બોલો\",\n",
    "    \"શાંતિ એ ખુબ સ્વાદિષ્ટ ભોજન પિરસીયું\",\n",
    "    \"ઋષિએ તપ કર્યું\",\n",
    "    \"તપ આજે જ ઘરે આવ્યો\",\n",
    "    \"વાહ શું વાત છે!\",\n",
    "    \"તમે વાહિયાત વાત કરો છો\",\n",
    "    \"તારે ફોનનું શું કામ છે?\",\n",
    "    \"કામ દેવ નો જન્મ શ્રીકૃષ્ણ અને રુક્મણી ને ત્યાં પ્રદ્યુમ્ન સ્વરૂપે જન્મ થયો\",\n",
    "    \"શ્રીકૃષ્ણ ખુબ સુંદર લાગે છે\",\n",
    "    \"સુંદર વાકાણી દયાનો સગો ભાઈ છે\",\n",
    "    \"મનીશે સાચી વાત કરી\",\n",
    "    \"સાચી ને ઊંઘમાં બોલવાની ટેવ છે\",\n",
    "    \"રવિ મોડો આવ્યો\",\n",
    "    \"કાલે રવિ વાર છે\",\n",
    "    \"નવરંગ ચૂંદડી માને શોભે\",\n",
    "    \"નવરંગ ચૂંદડી મા ને શોભે\",\n",
    "    \"નવ રંગ થી રંગોળી પાળી\"\n",
    "]\n",
    "\n",
    "for i,sentence in enumerate(sentences):\n",
    "  print(i+1,\".\",sentence)\n",
    "  print(model.infer(sentence))\n",
    "  print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJy9c3ry1HNf"
   },
   "source": [
    "# VGD3626/errant-for-gujarati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "QCVAJbQu2TTy"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet transformers==4.37\n",
    "!pip install --quiet sentence-transformers==2.7.0\n",
    "!pip install --quiet python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 14391,
     "status": "ok",
     "timestamp": 1750095227358,
     "user": {
      "displayName": "Vrund Dobariya",
      "userId": "02667739450293044164"
     },
     "user_tz": -330
    },
    "id": "Xpstg1PjYweN"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/VGD3626/errant-for-gujarati.git\n",
    "%cd errant-for-gujarati\n",
    "!pip install --quiet -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27550,
     "status": "ok",
     "timestamp": 1750095648728,
     "user": {
      "displayName": "Vrund Dobariya",
      "userId": "02667739450293044164"
     },
     "user_tz": -330
    },
    "id": "6mdNGrAFrAbn",
    "outputId": "aef9022a-a4b8-441c-f1f2-e3b31bb93f30"
   },
   "outputs": [],
   "source": [
    "# !errant_parallel -orig /content/org.txt -cor /content/hyp300.txt -out /content/hyp300.m2\n",
    "!errant_parallel -orig /content/org.txt -cor /content/cor_1.txt /content/cor_2.txt /content/cor_3.txt /content/cor_4.txt /content/cor_5.txt /content/cor_6.txt /content/cor_7.txt /content/cor_8.txt /content/cor_9.txt /content/cor_10.txt /content/cor_11.txt -out /content/gold300.m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13319,
     "status": "ok",
     "timestamp": 1750095662062,
     "user": {
      "displayName": "Vrund Dobariya",
      "userId": "02667739450293044164"
     },
     "user_tz": -330
    },
    "id": "RsatPdI_-IqH",
    "outputId": "2139b02f-ab6c-4840-e961-5b3a0df47de2"
   },
   "outputs": [],
   "source": [
    "!errant_compare -hyp /content/hyp300.m2 -ref /content/gold300.m2 -cat 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ArNZ_VkdCHaQ"
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1750095689399,
     "user": {
      "displayName": "Vrund Dobariya",
      "userId": "02667739450293044164"
     },
     "user_tz": -330
    },
    "id": "-PtBBJLVrBa3",
    "outputId": "c7e1027b-1b08-4883-b20e-7460d62b172b"
   },
   "outputs": [],
   "source": [
    "%cd /content/errant-for-gujarati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2JJsBW5rPli"
   },
   "outputs": [],
   "source": [
    "!git config --global user.email \"vrund3626@gmail.com\"\n",
    "!git config --global user.name \"VGD3626\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_GcgyvJNSwF"
   },
   "outputs": [],
   "source": [
    "!git remote set-url origin https://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@github.com/VGD3626/errant-for-gujarati.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1641,
     "status": "ok",
     "timestamp": 1750095696961,
     "user": {
      "displayName": "Vrund Dobariya",
      "userId": "02667739450293044164"
     },
     "user_tz": -330
    },
    "id": "OFhIZAIoLbFD",
    "outputId": "50e6a86a-b1ad-4141-cfca-856c8417d467"
   },
   "outputs": [],
   "source": [
    "!git add .\n",
    "# !git rm --cached -f errant-for-gujarati\n",
    "!git commit -m \"updated the classifier\"\n",
    "!git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1689,
     "status": "ok",
     "timestamp": 1749754600112,
     "user": {
      "displayName": "Vrund Dobariya",
      "userId": "02667739450293044164"
     },
     "user_tz": -330
    },
    "id": "3fJXMHQZOQSV",
    "outputId": "148a7f0a-a8cd-41dd-bfb1-fc3959558ff2"
   },
   "outputs": [],
   "source": [
    "# prompt: I have a file, named guj_gec.csv. read it using pandas and store sentence into c.txt and err_sentence into c.txt.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/content/drive/MyDrive/guj-gec.csv')\n",
    "\n",
    "# Assuming the column containing the sentences is named 'sentence' and the column with error sentences is 'err_sentence'\n",
    "# Replace 'sentence' and 'err_sentence' with the actual column names in your CSV file\n",
    "sentences_list = df['sentence'].tolist()\n",
    "err_sentences_list = df['err_sentence'].tolist()\n",
    "\n",
    "with open('/content/c.txt', 'w', encoding='utf-8') as f_c:\n",
    "    for sentence in sentences_list:\n",
    "        f_c.write(sentence + '\\n')\n",
    "\n",
    "with open('/content/o.txt', 'w', encoding='utf-8') as f_err:\n",
    "    for err_sentence in err_sentences_list:\n",
    "        f_err.write(err_sentence + '\\n')\n",
    "\n",
    "\n",
    "print(\"Sentences written to c.txt\")\n",
    "print(\"Error sentences written to err_sentence.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1749791271251,
     "user": {
      "displayName": "Vrund Dobariya",
      "userId": "02667739450293044164"
     },
     "user_tz": -330
    },
    "id": "TzifS9UjffEb",
    "outputId": "0e3b8c22-f28a-4fda-b697-4c0dada3efb2"
   },
   "outputs": [],
   "source": [
    "# prompt: write the code which reads the json file and write 'input' field into one text file and 'reference' in other text file.\n",
    "\n",
    "import json\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = '/content/15k_err_sentences.json' # Replace with the actual path\n",
    "\n",
    "# Specify the paths for the output text files\n",
    "input_output_path = '/content/input.txt'\n",
    "reference_output_path = '/content/reference.txt'\n",
    "\n",
    "try:\n",
    "    # Read the JSON file\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Open the output files for writing\n",
    "    with open(input_output_path, 'w', encoding='utf-8') as input_f, \\\n",
    "         open(reference_output_path, 'w', encoding='utf-8') as reference_f:\n",
    "\n",
    "        # Iterate through the data (assuming it's a list of dictionaries)\n",
    "        # If your JSON structure is different, you'll need to adjust this loop\n",
    "        if isinstance(data, list):\n",
    "            for item in data:\n",
    "                if 'input' in item:\n",
    "                    input_f.write(str(item['input']) + '\\n')\n",
    "                if 'reference' in item:\n",
    "                    reference_f.write(str(item['reference']) + '\\n')\n",
    "        elif isinstance(data, dict):\n",
    "            # If the JSON is a single dictionary\n",
    "            if 'input' in data:\n",
    "                input_f.write(str(data['input']) + '\\n')\n",
    "            if 'reference' in data:\n",
    "                reference_f.write(str(data['reference']) + '\\n')\n",
    "        else:\n",
    "            print(\"JSON structure not recognized. Please ensure it's a list or dictionary.\")\n",
    "\n",
    "\n",
    "    print(f\"'input' values written to {input_output_path}\")\n",
    "    print(f\"'reference' values written to {reference_output_path}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{json_file_path}' was not found.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Could not decode JSON from '{json_file_path}'. Please check the file format.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1749754600152,
     "user": {
      "displayName": "Vrund Dobariya",
      "userId": "02667739450293044164"
     },
     "user_tz": -330
    },
    "id": "TcO2PCuIlSwM",
    "outputId": "d45c56a2-e7d4-4ce9-e9ba-6c92fa2a27a3"
   },
   "outputs": [],
   "source": [
    "# prompt: read c.txt. Tokenize each line, combine tokens back, then write to the  file again. consider the case of punctuation while tokenizing\n",
    "\n",
    "import string\n",
    "\n",
    "def simple_tokenizer_with_punctuation(text):\n",
    "    \"\"\"\n",
    "    A function to tokenize a string into a list of words,\n",
    "    preserving punctuation as separate tokens.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of words and punctuation (tokens).\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    word = \"\"\n",
    "    for char in text:\n",
    "        if char.isspace():\n",
    "            if word:\n",
    "                tokens.append(word)\n",
    "            word = \"\"\n",
    "        elif char in string.punctuation:\n",
    "            if word:\n",
    "                tokens.append(word)\n",
    "            word = \"\"\n",
    "            tokens.append(char)\n",
    "        else:\n",
    "            word += char\n",
    "    if word:\n",
    "        tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "file_path = '/content/c.txt'\n",
    "processed_lines = []\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()  # Remove leading/trailing whitespace\n",
    "            if line:  # Process non-empty lines\n",
    "                tokens = simple_tokenizer_with_punctuation(line)\n",
    "                combined_line = ' '.join(tokens)\n",
    "                processed_lines.append(combined_line)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {file_path} was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Write the processed lines back to the file\n",
    "if processed_lines:\n",
    "    try:\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            for processed_line in processed_lines:\n",
    "                f.write(processed_line + '\\n')\n",
    "        print(f\"Successfully processed and wrote to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing to the file: {e}\")\n",
    "else:\n",
    "    print(f\"No lines were processed from {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "executionInfo": {
     "elapsed": 1538,
     "status": "ok",
     "timestamp": 1750183992432,
     "user": {
      "displayName": "Vrund Dobariya",
      "userId": "02667739450293044164"
     },
     "user_tz": -330
    },
    "id": "58TrVtdNWUEX",
    "outputId": "402e23f2-bcb3-4dc4-b18f-e088722d0824"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# categories = [\n",
    "#     \"ADJ\", \"ADJ:INFL\", \"ADP\", \"ADP:INFL\", \"AUX\", \"CONJ\", \"DET\", \"MORPH\", \"NOUN\",\n",
    "#     \"NOUN:INFL\", \"NUM\", \"ORTH\", \"OTHER\", \"PRON\", \"PRON:INFL\", \"PUNCT\", \"SPELL\",\n",
    "#     \"SPELL:ANUSVARA\", \"SPELL:MATRA\", \"VERB\", \"VERB:FORM\", \"VERB:INFL\", \"VERB:TENSE\", \"WO\"\n",
    "# ]\n",
    "\n",
    "# tp_counts = [\n",
    "#     20, 78, 4, 3, 37, 5, 6, 433, 86,\n",
    "#     493, 19, 167, 456, 46, 88, 67, 499,\n",
    "#     303, 535, 17, 4, 580, 408, 441\n",
    "# ]\n",
    "\n",
    "\n",
    "# categories = [\n",
    "#     \"ADJ\", \"ADJ:INFL\", \"ADP\", \"ADP:INFL\", \"AUX\", \"CONJ\", \"DET\", \"MORPH\", \"NOUN\",\n",
    "#     \"NOUN:INFL\", \"NUM\", \"ORTH\", \"OTHER\", \"PRON\", \"PRON:INFL\", \"PUNCT\", \"SPELL\",\n",
    "#     \"SPELL:ANUSVARA\", \"SPELL:MATRA\", \"VERB\", \"VERB:FORM\", \"VERB:INFL\", \"VERB:TENSE\", \"WO\", \"X\"\n",
    "# ]\n",
    "\n",
    "# tp_counts = [\n",
    "#     20, 78, 4, 3, 37, 5, 6, 433, 87,\n",
    "#     493, 19, 167, 453, 46, 88, 489, 72,\n",
    "#     303, 535, 17, 4, 580, 408, 441, 7\n",
    "# ]\n",
    "\n",
    "# my eval set\n",
    "# categories = [\n",
    "#     \"ADJ\", \"ADJ:INFL\", \"ADP\", \"ADV\", \"AUX\", \"CONJ\", \"DET\", \"MORPH\", \"NOUN\",\n",
    "#     \"NOUN:INFL\", \"NUM\", \"ORTH\", \"OTHER\", \"PART\", \"PRON\", \"PRON:INFL\", \"PUNCT\",\n",
    "#     \"SPELL\", \"SPELL:ANUSVARA\", \"SPELL:MATRA\", \"VERB\", \"VERB:INFL\", \"VERB:TENSE\", \"WO\", \"X\"\n",
    "# ]\n",
    "\n",
    "# tp_counts = [\n",
    "#     1, 14, 2, 2, 3, 5, 2, 19, 25,\n",
    "#     19, 8, 5, 52, 1, 8, 5, 91,\n",
    "#     62, 110, 38, 14, 52, 12, 18, 2\n",
    "# ]\n",
    "\n",
    "categories = [\n",
    "    \"ADJ\", \"ADJ:INFL\", \"ADP:INFL\", \"ADV\", \"CONJ\", \"DET\", \"MORPH\", \"NOUN\",\n",
    "    \"NOUN:INFL\", \"NUM\", \"ORTH\", \"OTHER\", \"PART\", \"PRON\", \"PRON:INFL\", \"PUNCT\",\n",
    "    \"SPELL\", \"SPELL:ANUSVARA\", \"SPELL:MATRA\", \"VERB\", \"VERB:INFL\", \"VERB:TENSE\", \"WO\"\n",
    "]\n",
    "\n",
    "tp_counts = [\n",
    "    1, 19, 2, 2, 5, 2, 21, 24,\n",
    "    27, 8, 5, 53, 1, 8, 5, 105,\n",
    "    50, 90, 35, 10, 68, 11, 18\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "percentages = [(tp / sum(tp_counts)) * 100 for tp in tp_counts]\n",
    "\n",
    "# Sort both lists together by percentage (descending)\n",
    "sorted_data = sorted(zip(percentages, categories), reverse=True)\n",
    "sorted_percentages, sorted_categories = zip(*sorted_data)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "bars = ax.bar(sorted_categories[:19], sorted_percentages[:19], color='mediumseagreen')\n",
    "ax.set_xticklabels(sorted_categories, rotation=45, ha='right')\n",
    "# ax.set_xlabel(\"Error Type\", fontsize=16)\n",
    "ax.set_ylabel(\"Samples (%)\", fontsize=14)\n",
    "# ax.set_title(\"Span-Based Correction: TP Distribution by Error Type (Percentage)\")\n",
    "\n",
    "# Show only x and y axis, no border\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Grid lines\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "ax.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Add percentage labels above bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.2f}%',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3),  # 3 points vertical offset\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "executionInfo": {
     "elapsed": 1739,
     "status": "ok",
     "timestamp": 1750183980782,
     "user": {
      "displayName": "Vrund Dobariya",
      "userId": "02667739450293044164"
     },
     "user_tz": -330
    },
    "id": "P4wNAx2QWU5A",
    "outputId": "6acb7265-c237-4006-e1b5-735b86bd0f88"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "categories = [\n",
    "    'ADJ', 'ADJ:INFL', 'ADP', 'ADP:INFL', 'ADV', 'AUX', 'CONJ', 'DET',\n",
    "    'MORPH', 'NOUN', 'NOUN:INFL', 'NUM', 'ORTH', 'OTHER', 'PART', 'PRON',\n",
    "    'PRON:INFL', 'PUNCT', 'SPELL:ANUSVARA', 'SPELL:MATRA', 'VERB', 'VERB:FORM',\n",
    "    'VERB:INFL', 'VERB:TENSE', 'WO'\n",
    "]\n",
    "tp_values = [\n",
    "    13, 60, 9, 5, 2, 24, 6, 6,\n",
    "    480, 126, 705, 19, 176, 527, 4, 41,\n",
    "    53, 461, 308, 701, 39, 5,\n",
    "    520, 216, 444\n",
    "]\n",
    "\n",
    "# Compute percentages\n",
    "total = sum(tp_values)\n",
    "percentages = [(tp / total) * 100 for tp in tp_values]\n",
    "\n",
    "# Sort both lists together by percentage (descending)\n",
    "sorted_data = sorted(zip(percentages, categories), reverse=True)\n",
    "sorted_percentages, sorted_categories = zip(*sorted_data)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "bars = ax.bar(sorted_categories[:19], sorted_percentages[:19], color='mediumseagreen')\n",
    "\n",
    "# Font sizes updated\n",
    "ax.set_xticklabels(sorted_categories, rotation=45, ha='right', fontsize=14)\n",
    "# ax.set_xlabel(\"Error Type\", fontsize=14)\n",
    "ax.set_ylabel(\"Samples (%)\", fontsize=14)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "\n",
    "# Show only x and y axis, no border\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Grid lines\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "ax.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Add percentage labels above bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.1f}%',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3),\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom',\n",
    "                fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "executionInfo": {
     "elapsed": 1313,
     "status": "ok",
     "timestamp": 1749922356850,
     "user": {
      "displayName": "Vrund Dobariya",
      "userId": "02667739450293044164"
     },
     "user_tz": -330
    },
    "id": "dF9O1cc8dWBI",
    "outputId": "43aaca9d-a6ad-4baa-8adb-20a500639289"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Original data\n",
    "categories = ['ADJ', 'ADJ:INFL', 'ADP', 'AUX', 'DET', 'MORPH', 'NOUN', 'NOUN:INFL',\n",
    "              'NUM', 'ORTH', 'OTHER', 'PART', 'PRON', 'PRON:INFL', 'PUNCT', 'SPELL',\n",
    "              'SPELL:ANUSVARA', 'SPELL:MATRA', 'VERB', 'VERB:INFL', 'VERB:TENSE',\n",
    "              'WO', 'X']\n",
    "\n",
    "TP =  [0, 10, 1, 3, 0, 13, 8, 13, 3, 4, 18, 1, 3, 2, 35, 26, 54, 25, 7, 36, 6, 1, 1]\n",
    "FP =  [2, 2, 0, 0, 1, 3, 8, 2, 0, 4, 24, 0, 4, 1, 18, 20, 12, 3, 4, 10, 4, 0, 0]\n",
    "FN =  [0, 1, 0, 0, 1, 2, 7, 1, 0, 2, 11, 0, 2, 1, 23, 14, 19, 5, 2, 6, 1, 2, 0]\n",
    "\n",
    "# Compute total errors per category\n",
    "total_errors = [tp + fp + fn for tp, fp, fn in zip(TP, FP, FN)]\n",
    "\n",
    "# Combine all into a list of tuples for sorting\n",
    "data = list(zip(categories, TP, FP, FN, total_errors))\n",
    "\n",
    "# Sort by total_errors descending\n",
    "data_sorted = sorted(data, key=lambda x: x[4], reverse=True)\n",
    "\n",
    "# Unpack sorted data\n",
    "categories_sorted, TP_sorted, FP_sorted, FN_sorted, _ = zip(*data_sorted[:19])\n",
    "\n",
    "# Convert to lists for later computations\n",
    "TP_sorted = list(TP_sorted)\n",
    "FP_sorted = list(FP_sorted)\n",
    "FN_sorted = list(FN_sorted)\n",
    "\n",
    "# Compute percentages\n",
    "totals_sorted = [tp + fp + fn for tp, fp, fn in zip(TP_sorted, FP_sorted, FN_sorted)]\n",
    "corrected = [tp/tot*100 for tp, tot in zip(TP_sorted, totals_sorted)]\n",
    "unchanged = [fn/tot*100 for fn, tot in zip(FN_sorted, totals_sorted)]\n",
    "incorrect = [fp/tot*100 for fp, tot in zip(FP_sorted, totals_sorted)]\n",
    "\n",
    "\n",
    "ind = np.arange(len(categories_sorted))\n",
    "width = 0.7\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "p1 = plt.bar(ind, corrected, width, label='Corrected', color='#76c7c0')\n",
    "p2 = plt.bar(ind, unchanged, width, bottom=corrected, label='Unchanged', color='#ffcc00')\n",
    "p3 = plt.bar(ind, incorrect, width, bottom=[i+j for i,j in zip(corrected, unchanged)], label='Incorrect', color='#ff6f69')\n",
    "\n",
    "plt.ylabel('Percentage', fontsize=16)\n",
    "# plt.title('Span-Based Correction Performance Sorted by Error Composition', fontsize=18)\n",
    "plt.xticks(ind, categories_sorted, rotation=45, ha='right', fontsize=16)\n",
    "plt.yticks(np.arange(0, 101, 10), fontsize=16)\n",
    "plt.legend(fontsize=14, loc='upper right', ncol=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7BfV5TDgTVZv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "plVRzGvD1BuB"
   ],
   "gpuType": "T4",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
